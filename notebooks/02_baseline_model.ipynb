{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b4919",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a61dcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model_name': 'simple_cnn',\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 30,\n",
    "    'image_size': 48,\n",
    "    'num_classes': 7,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"facial-expression-recognition\",\n",
    "    name=f\"{CONFIG['model_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    config=CONFIG,\n",
    "    job_type=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6202101",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f6bff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, indices, transform=None):\n",
    "        self.data = dataframe.iloc[indices].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get pixel data and convert to image\n",
    "        pixels = self.data.iloc[idx]['pixels']\n",
    "        image = np.array(pixels.split(), dtype=np.uint8).reshape(48, 48)\n",
    "        \n",
    "        # Convert to PIL format for transforms\n",
    "        image = image.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
    "        image = torch.from_numpy(image).unsqueeze(0)  # Add channel dimension\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = int(self.data.iloc[idx]['emotion'])\n",
    "        \n",
    "        return image, label"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
