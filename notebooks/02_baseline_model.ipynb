{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e911366d",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/Facial-Expression-Recognition/blob/main/notebooks/02_baseline_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "wbo7YhsKtC9W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbo7YhsKtC9W",
        "outputId": "e16407fd-b2e0-43ea-c1c3-8e2f518ff826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "-bO7jJJStOhQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-bO7jJJStOhQ",
        "outputId": "3139c3fc-f4c1-429f-d10f-ee7a7559a671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cufft-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mcp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d8C1uZCRtaUB",
      "metadata": {
        "id": "d8C1uZCRtaUB"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "grXZd0wAtmhA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grXZd0wAtmhA",
        "outputId": "c4eb42f2-1505-49a2-f85d-cf56895e495b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 88% 250M/285M [00:00<00:00, 408MB/s]\n",
            "100% 285M/285M [00:00<00:00, 421MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "644b4919",
      "metadata": {
        "id": "644b4919"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import wandb\n",
        "import time\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "06a61dcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "06a61dcf",
        "outputId": "188f26d5-214f-403a-86b2-fb39e9df0159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mellekvirikashvili\u001b[0m (\u001b[33mellekvirikashvili-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250530_101616-70toflci</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition/runs/70toflci' target=\"_blank\">simple_cnn_20250530_101609</a></strong> to <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition' target=\"_blank\">https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition/runs/70toflci' target=\"_blank\">https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition/runs/70toflci</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition/runs/70toflci?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7c64d061be50>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'model_name': 'simple_cnn',\n",
        "    'batch_size': 32,\n",
        "    'learning_rate': 0.001,\n",
        "    'epochs': 30,\n",
        "    'image_size': 48,\n",
        "    'num_classes': 7,\n",
        "    'random_seed': 42\n",
        "}\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(CONFIG['random_seed'])\n",
        "np.random.seed(CONFIG['random_seed'])\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(\n",
        "    project=\"facial-expression-recognition\",\n",
        "    name=f\"{CONFIG['model_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
        "    config=CONFIG,\n",
        "    job_type=\"training\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6202101",
      "metadata": {
        "id": "f6202101"
      },
      "source": [
        "# Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "953f6bff",
      "metadata": {
        "id": "953f6bff"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class FERDataset(Dataset):\n",
        "    def __init__(self, dataframe, indices, transform=None):\n",
        "        self.data = dataframe.iloc[indices].reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get pixel data and convert to image\n",
        "        pixels = self.data.iloc[idx]['pixels']\n",
        "        image = np.array(pixels.split(), dtype=np.uint8).reshape(48, 48)\n",
        "\n",
        "        # Convert to PIL format for transforms\n",
        "        image = image.astype(np.float32) / 255.0  # Normalize to [0,1]\n",
        "        image = torch.from_numpy(image).unsqueeze(0)  # Add channel dimension\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = int(self.data.iloc[idx]['emotion'])\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7UeJV6F3sdsT",
      "metadata": {
        "id": "7UeJV6F3sdsT"
      },
      "source": [
        "#  Define Simple CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "oXPMjtHIscVk",
      "metadata": {
        "id": "oXPMjtHIscVk"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Calculate the size after convolutions and pooling\n",
        "        # 48 -> 24 -> 12 -> 6 after three pooling operations\n",
        "        self.fc1 = nn.Linear(128 * 6 * 6, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv block 1\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 48x48 -> 24x24\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 24x24 -> 12x12\n",
        "\n",
        "        # Conv block 3\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 12x12 -> 6x6\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eZ7laA4skVN",
      "metadata": {
        "id": "3eZ7laA4skVN"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "xqtIpz3fsiu6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqtIpz3fsiu6",
        "outputId": "fe81a008-ee40-452a-dc3d-d946d9fc3e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Training samples: 22967\n",
            "Validation samples: 5742\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "train_df = pd.read_csv('train.csv')\n",
        "train_indices, val_indices = train_test_split(\n",
        "    range(len(train_df)),\n",
        "    test_size=0.2,\n",
        "    stratify=train_df['emotion'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# Create datasets (no augmentation for baseline)\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "train_dataset = FERDataset(train_df, train_indices, transform=train_transform)\n",
        "val_dataset = FERDataset(train_df, val_indices, transform=val_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cTdPZV0AuTN4",
      "metadata": {
        "id": "cTdPZV0AuTN4"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zsdkLiTAuIob",
      "metadata": {
        "id": "zsdkLiTAuIob"
      },
      "source": [
        "## Initialize model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "mBxY-pASuHJc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBxY-pASuHJc",
        "outputId": "f7198ead-2c07-47c2-9503-887498e6af45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Parameters: 2,456,071 total, 2,456,071 trainable\n"
          ]
        }
      ],
      "source": [
        "model = SimpleCNN(num_classes=CONFIG['num_classes']).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model Parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
        "\n",
        "# Log model info\n",
        "wandb.log({\n",
        "    \"model_parameters\": total_params,\n",
        "    \"trainable_parameters\": trainable_params\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "WtGaIeY9uL-_",
      "metadata": {
        "id": "WtGaIeY9uL-_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RRkO-zF0uO2-",
      "metadata": {
        "id": "RRkO-zF0uO2-"
      },
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dt9Up7vHuOZO",
      "metadata": {
        "id": "dt9Up7vHuOZO"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y9hU3Z69uYg-",
      "metadata": {
        "id": "y9hU3Z69uYg-"
      },
      "source": [
        "## Validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bZ8_GPuyuYEU",
      "metadata": {
        "id": "bZ8_GPuyuYEU"
      },
      "outputs": [],
      "source": [
        "# Validation function\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc, all_preds, all_targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zFUubbEFueMn",
      "metadata": {
        "id": "zFUubbEFueMn"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "JB7-quGnudYv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JB7-quGnudYv",
        "outputId": "850b3ce4-e49d-4b52-cf3c-38998dc85b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "Batch 0/718, Loss: 1.9443\n",
            "Batch 100/718, Loss: 1.8132\n",
            "Batch 200/718, Loss: 1.5460\n",
            "Batch 300/718, Loss: 1.5465\n",
            "Batch 400/718, Loss: 1.8359\n",
            "Batch 500/718, Loss: 1.6811\n",
            "Batch 600/718, Loss: 1.4701\n",
            "Batch 700/718, Loss: 1.5407\n",
            "Epoch [1/30]\n",
            "Train Loss: 1.6369, Train Acc: 35.04%\n",
            "Val Loss: 1.4597, Val Acc: 44.03%\n",
            "Time: 204.96s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 44.03%\n",
            "Batch 0/718, Loss: 1.5783\n",
            "Batch 100/718, Loss: 1.4506\n",
            "Batch 200/718, Loss: 1.2007\n",
            "Batch 300/718, Loss: 1.3132\n",
            "Batch 400/718, Loss: 1.4726\n",
            "Batch 500/718, Loss: 1.2295\n",
            "Batch 600/718, Loss: 1.2094\n",
            "Batch 700/718, Loss: 1.6433\n",
            "Epoch [2/30]\n",
            "Train Loss: 1.3985, Train Acc: 46.47%\n",
            "Val Loss: 1.3030, Val Acc: 50.23%\n",
            "Time: 194.68s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 50.23%\n",
            "Batch 0/718, Loss: 1.4325\n",
            "Batch 100/718, Loss: 1.1674\n",
            "Batch 200/718, Loss: 0.9965\n",
            "Batch 300/718, Loss: 1.3791\n",
            "Batch 400/718, Loss: 1.2431\n",
            "Batch 500/718, Loss: 1.4038\n",
            "Batch 600/718, Loss: 1.2080\n",
            "Batch 700/718, Loss: 1.2955\n",
            "Epoch [3/30]\n",
            "Train Loss: 1.2620, Train Acc: 51.87%\n",
            "Val Loss: 1.2240, Val Acc: 53.20%\n",
            "Time: 197.15s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 53.20%\n",
            "Batch 0/718, Loss: 1.0759\n",
            "Batch 100/718, Loss: 1.3135\n",
            "Batch 200/718, Loss: 1.1178\n",
            "Batch 300/718, Loss: 0.6414\n",
            "Batch 400/718, Loss: 1.2250\n",
            "Batch 500/718, Loss: 1.0085\n",
            "Batch 600/718, Loss: 1.5921\n",
            "Batch 700/718, Loss: 1.2364\n",
            "Epoch [4/30]\n",
            "Train Loss: 1.1586, Train Acc: 55.95%\n",
            "Val Loss: 1.1619, Val Acc: 55.96%\n",
            "Time: 202.13s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 55.96%\n",
            "Batch 0/718, Loss: 0.8726\n",
            "Batch 100/718, Loss: 0.9900\n",
            "Batch 200/718, Loss: 1.2128\n",
            "Batch 300/718, Loss: 1.0367\n",
            "Batch 400/718, Loss: 0.9361\n",
            "Batch 500/718, Loss: 1.0963\n",
            "Batch 600/718, Loss: 1.2163\n",
            "Batch 700/718, Loss: 0.8240\n",
            "Epoch [5/30]\n",
            "Train Loss: 1.0657, Train Acc: 60.02%\n",
            "Val Loss: 1.1551, Val Acc: 56.91%\n",
            "Time: 193.03s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 56.91%\n",
            "Batch 0/718, Loss: 0.9575\n",
            "Batch 100/718, Loss: 0.9170\n",
            "Batch 200/718, Loss: 1.2196\n",
            "Batch 300/718, Loss: 1.2655\n",
            "Batch 400/718, Loss: 0.7493\n",
            "Batch 500/718, Loss: 0.9106\n",
            "Batch 600/718, Loss: 1.1958\n",
            "Batch 700/718, Loss: 1.0625\n",
            "Epoch [6/30]\n",
            "Train Loss: 0.9669, Train Acc: 63.86%\n",
            "Val Loss: 1.1451, Val Acc: 57.42%\n",
            "Time: 190.06s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 57.42%\n",
            "Batch 0/718, Loss: 0.5684\n",
            "Batch 100/718, Loss: 0.9152\n",
            "Batch 200/718, Loss: 0.7333\n",
            "Batch 300/718, Loss: 0.8227\n",
            "Batch 400/718, Loss: 1.0607\n",
            "Batch 500/718, Loss: 0.8256\n",
            "Batch 600/718, Loss: 0.8689\n",
            "Batch 700/718, Loss: 0.9528\n",
            "Epoch [7/30]\n",
            "Train Loss: 0.8674, Train Acc: 67.65%\n",
            "Val Loss: 1.1550, Val Acc: 57.66%\n",
            "Time: 194.48s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 57.66%\n",
            "Batch 0/718, Loss: 0.8847\n",
            "Batch 100/718, Loss: 0.6771\n",
            "Batch 200/718, Loss: 0.9553\n",
            "Batch 300/718, Loss: 0.5423\n",
            "Batch 400/718, Loss: 0.9514\n",
            "Batch 500/718, Loss: 1.0049\n",
            "Batch 600/718, Loss: 0.9237\n",
            "Batch 700/718, Loss: 0.6918\n",
            "Epoch [8/30]\n",
            "Train Loss: 0.7691, Train Acc: 71.27%\n",
            "Val Loss: 1.2434, Val Acc: 57.04%\n",
            "Time: 189.93s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.7219\n",
            "Batch 100/718, Loss: 0.4457\n",
            "Batch 200/718, Loss: 0.6325\n",
            "Batch 300/718, Loss: 0.6314\n",
            "Batch 400/718, Loss: 0.7462\n",
            "Batch 500/718, Loss: 0.6540\n",
            "Batch 600/718, Loss: 0.5787\n",
            "Batch 700/718, Loss: 0.7659\n",
            "Epoch [9/30]\n",
            "Train Loss: 0.6779, Train Acc: 74.81%\n",
            "Val Loss: 1.2630, Val Acc: 57.89%\n",
            "Time: 191.69s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 57.89%\n",
            "Batch 0/718, Loss: 0.6808\n",
            "Batch 100/718, Loss: 0.3597\n",
            "Batch 200/718, Loss: 0.6145\n",
            "Batch 300/718, Loss: 0.5058\n",
            "Batch 400/718, Loss: 0.6764\n",
            "Batch 500/718, Loss: 0.9098\n",
            "Batch 600/718, Loss: 0.5425\n",
            "Batch 700/718, Loss: 0.8417\n",
            "Epoch [10/30]\n",
            "Train Loss: 0.6000, Train Acc: 78.02%\n",
            "Val Loss: 1.3588, Val Acc: 57.04%\n",
            "Time: 195.64s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.4882\n",
            "Batch 100/718, Loss: 0.5003\n",
            "Batch 200/718, Loss: 0.4423\n",
            "Batch 300/718, Loss: 0.3122\n",
            "Batch 400/718, Loss: 0.5329\n",
            "Batch 500/718, Loss: 0.5881\n",
            "Batch 600/718, Loss: 0.6023\n",
            "Batch 700/718, Loss: 0.5080\n",
            "Epoch [11/30]\n",
            "Train Loss: 0.5199, Train Acc: 80.64%\n",
            "Val Loss: 1.4490, Val Acc: 56.69%\n",
            "Time: 191.69s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.4885\n",
            "Batch 100/718, Loss: 0.3111\n",
            "Batch 200/718, Loss: 0.3535\n",
            "Batch 300/718, Loss: 0.2840\n",
            "Batch 400/718, Loss: 0.4659\n",
            "Batch 500/718, Loss: 0.6118\n",
            "Batch 600/718, Loss: 0.4588\n",
            "Batch 700/718, Loss: 0.4928\n",
            "Epoch [12/30]\n",
            "Train Loss: 0.4605, Train Acc: 83.08%\n",
            "Val Loss: 1.5351, Val Acc: 58.24%\n",
            "Time: 187.73s\n",
            "--------------------------------------------------\n",
            "New best validation accuracy: 58.24%\n",
            "Batch 0/718, Loss: 0.3405\n",
            "Batch 100/718, Loss: 0.4471\n",
            "Batch 200/718, Loss: 0.4161\n",
            "Batch 300/718, Loss: 0.4855\n",
            "Batch 400/718, Loss: 0.4120\n",
            "Batch 500/718, Loss: 0.3200\n",
            "Batch 600/718, Loss: 0.5389\n",
            "Batch 700/718, Loss: 0.8094\n",
            "Epoch [13/30]\n",
            "Train Loss: 0.4077, Train Acc: 85.03%\n",
            "Val Loss: 1.5899, Val Acc: 57.87%\n",
            "Time: 189.07s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.2413\n",
            "Batch 100/718, Loss: 0.2879\n",
            "Batch 200/718, Loss: 0.5958\n",
            "Batch 300/718, Loss: 0.3554\n",
            "Batch 400/718, Loss: 0.1909\n",
            "Batch 500/718, Loss: 0.3412\n",
            "Batch 600/718, Loss: 0.3080\n",
            "Batch 700/718, Loss: 0.2267\n",
            "Epoch [14/30]\n",
            "Train Loss: 0.3714, Train Acc: 86.20%\n",
            "Val Loss: 1.7632, Val Acc: 57.82%\n",
            "Time: 190.37s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.3104\n",
            "Batch 100/718, Loss: 0.3435\n",
            "Batch 200/718, Loss: 0.4839\n",
            "Batch 300/718, Loss: 0.2310\n",
            "Batch 400/718, Loss: 0.2136\n",
            "Batch 500/718, Loss: 0.6072\n",
            "Batch 600/718, Loss: 0.1933\n",
            "Batch 700/718, Loss: 0.4163\n",
            "Epoch [15/30]\n",
            "Train Loss: 0.3445, Train Acc: 87.56%\n",
            "Val Loss: 1.7993, Val Acc: 57.18%\n",
            "Time: 191.16s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.1978\n",
            "Batch 100/718, Loss: 0.1650\n",
            "Batch 200/718, Loss: 0.3293\n",
            "Batch 300/718, Loss: 0.2830\n",
            "Batch 400/718, Loss: 0.3178\n",
            "Batch 500/718, Loss: 0.4728\n",
            "Batch 600/718, Loss: 0.2674\n",
            "Batch 700/718, Loss: 0.2966\n",
            "Epoch [16/30]\n",
            "Train Loss: 0.3112, Train Acc: 88.62%\n",
            "Val Loss: 1.8024, Val Acc: 57.28%\n",
            "Time: 193.05s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.3273\n",
            "Batch 100/718, Loss: 0.1589\n",
            "Batch 200/718, Loss: 0.2113\n",
            "Batch 300/718, Loss: 0.2813\n",
            "Batch 400/718, Loss: 0.2039\n",
            "Batch 500/718, Loss: 0.2066\n",
            "Batch 600/718, Loss: 0.3014\n",
            "Batch 700/718, Loss: 0.3018\n",
            "Epoch [17/30]\n",
            "Train Loss: 0.2815, Train Acc: 89.72%\n",
            "Val Loss: 1.9476, Val Acc: 56.76%\n",
            "Time: 191.59s\n",
            "--------------------------------------------------\n",
            "Batch 0/718, Loss: 0.1404\n",
            "Batch 100/718, Loss: 0.2933\n",
            "Batch 200/718, Loss: 0.2813\n",
            "Batch 300/718, Loss: 0.1510\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5889c55f51db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-f0a2f86e145c>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_as_effectful_op_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallthrough_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"\\nStarting training...\")\n",
        "train_losses, train_accs = [], []\n",
        "val_losses, val_accs = [], []\n",
        "best_val_acc = 0.0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(CONFIG['epochs']):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc, val_preds, val_targets = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_accuracy': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_accuracy': val_acc,\n",
        "        'epoch_time': epoch_time,\n",
        "        'learning_rate': optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{CONFIG[\"epochs\"]}]')\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    print(f'Time: {epoch_time:.2f}s')\n",
        "    print('-' * 50)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'simple_cnn_best.pth')\n",
        "        print(f'New best validation accuracy: {best_val_acc:.2f}%')\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f'\\nTraining completed in {total_time:.2f}s')\n",
        "print(f'Best validation accuracy: {best_val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yaLkuDoWuj4y",
      "metadata": {
        "id": "yaLkuDoWuj4y"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "plt.plot(val_losses, label='Val Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_accs, label='Train Acc', color='blue')\n",
        "plt.plot(val_accs, label='Val Acc', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(np.array(train_accs) - np.array(val_accs), label='Acc Gap', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Train - Val Accuracy (%)')\n",
        "plt.title('Overfitting Indicator')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('simple_cnn_training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jOyq8NG2usFF",
      "metadata": {
        "id": "jOyq8NG2usFF"
      },
      "source": [
        "## load bst model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2UWCjKHgup88",
      "metadata": {
        "id": "2UWCjKHgup88"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load best model for final evaluation\n",
        "model.load_state_dict(torch.load('simple_cnn_best.pth'))\n",
        "\n",
        "# Final validation with best model\n",
        "final_val_loss, final_val_acc, final_preds, final_targets = validate_epoch(\n",
        "    model, val_loader, criterion, device\n",
        ")\n",
        "\n",
        "print(f'Final validation accuracy: {final_val_acc:.2f}%')\n",
        "\n",
        "# Classification report\n",
        "expression_mapping = {\n",
        "    0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy',\n",
        "    4: 'Sad', 5: 'Surprise', 6: 'Neutral'\n",
        "}\n",
        "\n",
        "class_names = [expression_mapping[i] for i in range(7)]\n",
        "class_report = classification_report(\n",
        "    final_targets, final_preds,\n",
        "    target_names=class_names,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(final_targets, final_preds, target_names=class_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3CuKlTInuwtc",
      "metadata": {
        "id": "3CuKlTInuwtc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(final_targets, final_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix - Simple CNN')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "plt.savefig('simple_cnn_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Analyze per-class performance\n",
        "per_class_acc = []\n",
        "for i in range(7):\n",
        "    class_mask = np.array(final_targets) == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_acc = (np.array(final_preds)[class_mask] == i).mean() * 100\n",
        "        per_class_acc.append(class_acc)\n",
        "        print(f'{class_names[i]}: {class_acc:.1f}% ({class_mask.sum()} samples)')\n",
        "\n",
        "# Log final results to wandb\n",
        "wandb.log({\n",
        "    'final_val_accuracy': final_val_acc,\n",
        "    'final_val_loss': final_val_loss,\n",
        "    'training_curves': wandb.Image('simple_cnn_training_curves.png'),\n",
        "    'confusion_matrix': wandb.Image('simple_cnn_confusion_matrix.png'),\n",
        "    'classification_report': class_report,\n",
        "    'total_training_time': total_time,\n",
        "    'best_epoch': np.argmax(val_accs) + 1\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DjDei671uxmM",
      "metadata": {
        "id": "DjDei671uxmM"
      },
      "source": [
        "# Analyzis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6lj0uRwsu0qF",
      "metadata": {
        "id": "6lj0uRwsu0qF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Analysis of results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SIMPLE CNN BASELINE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check for overfitting/underfitting\n",
        "final_train_acc = train_accs[-1]\n",
        "acc_gap = final_train_acc - final_val_acc\n",
        "\n",
        "print(f\"\\nOverfitting Analysis:\")\n",
        "print(f\"Final Training Accuracy: {final_train_acc:.2f}%\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.2f}%\")\n",
        "print(f\"Accuracy Gap: {acc_gap:.2f}%\")\n",
        "\n",
        "if acc_gap > 10:\n",
        "    print(\"🔴 OVERFITTING DETECTED: Large gap between train and validation accuracy\")\n",
        "    print(\"   - Model memorizing training data\")\n",
        "    print(\"   - Need regularization (dropout, data augmentation)\")\n",
        "elif acc_gap < 5:\n",
        "    print(\"🟡 GOOD GENERALIZATION: Small gap between train and validation\")\n",
        "    if final_val_acc < 50:\n",
        "        print(\"🔴 UNDERFITTING: Both accuracies are low\")\n",
        "        print(\"   - Model too simple for the task\")\n",
        "        print(\"   - Need more capacity or different architecture\")\n",
        "    else:\n",
        "        print(\"✅ BALANCED MODEL: Good generalization\")\n",
        "else:\n",
        "    print(\"🟡 MILD OVERFITTING: Moderate gap, could be improved\")\n",
        "\n",
        "# Learning curve analysis\n",
        "print(f\"\\nLearning Curve Analysis:\")\n",
        "print(f\"Best epoch: {np.argmax(val_accs) + 1}\")\n",
        "print(f\"Early stopping could have saved {CONFIG['epochs'] - np.argmax(val_accs) - 1} epochs\")\n",
        "\n",
        "if val_accs[-1] < max(val_accs) * 0.95:\n",
        "    print(\"🔴 VALIDATION ACCURACY DECLINING: Clear overfitting in later epochs\")\n",
        "else:\n",
        "    print(\"✅ STABLE LEARNING: No significant decline in validation performance\")\n",
        "\n",
        "# Performance insights\n",
        "worst_class = class_names[np.argmin(per_class_acc)]\n",
        "best_class = class_names[np.argmax(per_class_acc)]\n",
        "\n",
        "print(f\"\\nPer-class Performance:\")\n",
        "print(f\"Best performing class: {best_class} ({max(per_class_acc):.1f}%)\")\n",
        "print(f\"Worst performing class: {worst_class} ({min(per_class_acc):.1f}%)\")\n",
        "print(f\"Performance variance: {np.std(per_class_acc):.1f}%\")\n",
        "\n",
        "# Model complexity analysis\n",
        "print(f\"\\nModel Complexity:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB (float32)\")\n",
        "print(f\"Training time: {total_time:.1f}s ({total_time/CONFIG['epochs']:.1f}s/epoch)\")\n",
        "\n",
        "# Recommendations for next experiments\n",
        "print(f\"\\n🔬 RECOMMENDATIONS FOR NEXT EXPERIMENTS:\")\n",
        "print(f\"1. Architecture: Try deeper CNN with more layers\")\n",
        "print(f\"2. Regularization: Add batch normalization, more dropout\")\n",
        "print(f\"3. Data: Implement data augmentation to reduce overfitting\")\n",
        "print(f\"4. Optimization: Try different learning rates, schedulers\")\n",
        "print(f\"5. Loss: Consider weighted loss for class imbalance\")\n",
        "\n",
        "# Save experiment summary\n",
        "experiment_summary = {\n",
        "    'model_name': CONFIG['model_name'],\n",
        "    'final_val_accuracy': final_val_acc,\n",
        "    'best_val_accuracy': best_val_acc,\n",
        "    'final_train_accuracy': final_train_acc,\n",
        "    'overfitting_gap': acc_gap,\n",
        "    'total_parameters': total_params,\n",
        "    'training_time': total_time,\n",
        "    'best_epoch': int(np.argmax(val_accs) + 1),\n",
        "    'per_class_accuracy': dict(zip(class_names, per_class_acc)),\n",
        "    'key_findings': {\n",
        "        'overfitting_detected': acc_gap > 10,\n",
        "        'underfitting_detected': final_val_acc < 40,\n",
        "        'early_stopping_beneficial': val_accs[-1] < max(val_accs) * 0.95,\n",
        "        'class_imbalance_impact': max(per_class_acc) - min(per_class_acc) > 20\n",
        "    }\n",
        "}\n",
        "\n",
        "# Log summary\n",
        "wandb.log({'experiment_summary': experiment_summary})\n",
        "\n",
        "print(f\"\\n✅ Simple CNN baseline complete!\")\n",
        "print(f\"📊 Results logged to Wandb\")\n",
        "print(f\"💾 Best model saved as 'simple_cnn_best.pth'\")\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AGXAmiwwu8Mf",
      "metadata": {
        "id": "AGXAmiwwu8Mf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Template for README documentation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DOCUMENTATION FOR README:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\"\"\n",
        "## Experiment 1: Simple CNN Baseline\n",
        "\n",
        "### Hypothesis\n",
        "Start with a minimal CNN architecture to establish a baseline performance.\n",
        "Expected: Likely to underfit due to limited model capacity for complex facial expressions.\n",
        "\n",
        "### Architecture\n",
        "```python\n",
        "SimpleCNN(\n",
        "  (conv1): Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "  (conv2): Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "  (conv3): Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "  (pool): MaxPool2d(2, 2)\n",
        "  (dropout): Dropout(0.5)\n",
        "  (fc1): Linear(4608, 512)\n",
        "  (fc2): Linear(512, 7)\n",
        ")\n",
        "```\n",
        "- **Parameters**: {total_params:,}\n",
        "- **Layers**: 3 conv + 2 FC layers\n",
        "- **Regularization**: Dropout (0.5)\n",
        "\n",
        "### Hyperparameters\n",
        "- **Learning Rate**: {CONFIG['learning_rate']}\n",
        "- **Batch Size**: {CONFIG['batch_size']}\n",
        "- **Epochs**: {CONFIG['epochs']}\n",
        "- **Optimizer**: Adam\n",
        "- **Loss**: CrossEntropyLoss\n",
        "\n",
        "### Results\n",
        "- **Best Validation Accuracy**: {best_val_acc:.2f}%\n",
        "- **Final Training Accuracy**: {final_train_acc:.2f}%\n",
        "- **Overfitting Gap**: {acc_gap:.2f}%\n",
        "- **Training Time**: {total_time:.1f}s\n",
        "\n",
        "### Analysis\n",
        "{'🔴 **OVERFITTING DETECTED**' if acc_gap > 10 else '🟡 **MILD OVERFITTING**' if acc_gap > 5 else '✅ **GOOD GENERALIZATION**'}\n",
        "- Training accuracy ({final_train_acc:.1f}%) {'significantly higher than' if acc_gap > 10 else 'moderately higher than' if acc_gap > 5 else 'close to'} validation ({final_val_acc:.1f}%)\n",
        "- {'Model is memorizing training data rather than learning generalizable features' if acc_gap > 10 else 'Some overfitting present but manageable' if acc_gap > 5 else 'Good balance between fitting and generalization'}\n",
        "\n",
        "**Per-class Performance**:\n",
        "- Best: {best_class} ({max(per_class_acc):.1f}%)\n",
        "- Worst: {worst_class} ({min(per_class_acc):.1f}%)\n",
        "- High variance ({np.std(per_class_acc):.1f}%) suggests class imbalance impact\n",
        "\n",
        "### Key Findings\n",
        "1. **Baseline Established**: {final_val_acc:.1f}% accuracy provides lower bound\n",
        "2. **{'Overfitting' if acc_gap > 10 else 'Underfitting' if final_val_acc < 40 else 'Balanced'} Detected**: {'Need regularization techniques' if acc_gap > 10 else 'Need more model capacity' if final_val_acc < 40 else 'Good starting point'}\n",
        "3. **Class Imbalance Impact**: {max(per_class_acc) - min(per_class_acc):.1f}% performance gap\n",
        "4. **Training Efficiency**: {total_time/CONFIG['epochs']:.1f}s per epoch, converged around epoch {np.argmax(val_accs) + 1}\n",
        "\n",
        "### Next Steps\n",
        "1. **Regularization**: Add batch normalization, data augmentation\n",
        "2. **Architecture**: Increase depth and width gradually\n",
        "3. **Data Strategy**: Address class imbalance with weighted loss\n",
        "4. **Optimization**: Experiment with learning rate scheduling\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
