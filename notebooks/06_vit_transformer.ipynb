{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affa1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wandb torch torchvision pandas numpy matplotlib seaborn\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "!unzip -q challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e85876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import ViTForImageClassification\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63424759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e8802",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FER2013Dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image and label\n",
    "        pixels = self.data.iloc[idx]['pixels']\n",
    "        image = np.array(pixels.split(), dtype='uint8')\n",
    "        image = image.reshape(48, 48, 1).astype('float32') / 255.0\n",
    "        \n",
    "        # Convert to 3 channels for ViT\n",
    "        image = np.repeat(image, 3, axis=-1)\n",
    "        \n",
    "        label = self.data.iloc[idx]['emotion']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba084541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and prepare data\n",
    "print(\"Loading data...\")\n",
    "data_path = 'train.csv'  # Update this path\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c96cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['emotion'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['emotion'])\n",
    "\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = FER2013Dataset(train_df, train_transform)\n",
    "val_dataset = FER2013Dataset(val_df, val_transform)\n",
    "test_dataset = FER2013Dataset(test_df, val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845eff41",
   "metadata": {},
   "source": [
    "# 3. Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea926bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Model Definition\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes=7, model_name='google/vit-base-patch16-224'):\n",
    "        super().__init__()\n",
    "        self.model = ViTForImageClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = VisionTransformer(num_classes=7).to(device)\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bafd61",
   "metadata": {},
   "source": [
    "# 4. Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09afb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * 20  # 20 epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78557cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model and training configuration\n",
    "config = {\n",
    "    \"model_name\": \"google/vit-base-patch16-224\",\n",
    "    \"num_classes\": 7,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 20,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"image_size\": 224,\n",
    "    \"dataset\": \"FER2013\",\n",
    "    \"architecture\": \"VisionTransformer\",\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"CosineWithWarmup\",\n",
    "    \"loss_function\": \"CrossEntropyLoss\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c32029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Weights & Biases with more comprehensive config\n",
    "wandb.init(\n",
    "    project=\"facial-expression-recognition\",\n",
    "    config=config,\n",
    "    name=f\"vit-fer2013-{wandb.util.generate_id()}\",\n",
    "    tags=[\"vision-transformer\", \"fer2013\", \"facial-expression-recognition\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log dataset statistics\n",
    "class_counts = df['emotion'].value_counts().to_dict()\n",
    "wandb.log({\"class_distribution\": wandb.plot.bar(\n",
    "    wandb.Table(\n",
    "        data=[[k, v] for k, v in class_counts.items()],\n",
    "        columns=[\"class\", \"count\"]\n",
    "    ),\n",
    "    \"class\",\n",
    "    \"count\",\n",
    "    title=\"Class Distribution\"\n",
    ")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Training Loop\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for inputs, labels in pbar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / total,\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = 100. * correct / len(dataloader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f16f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Validation\")\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / total,\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_acc = 100. * correct / len(dataloader.dataset)\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e2f63",
   "metadata": {},
   "source": [
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307bf875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(f\"\\nEpoch {epoch+1}/20\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device, scheduler\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = validate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Log metrics\n",
    "    metrics = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/accuracy\": train_acc,\n",
    "        \"val/loss\": val_loss,\n",
    "        \"val/accuracy\": val_acc,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "    }\n",
    "    \n",
    "    # Log some sample predictions every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        # Get sample predictions\n",
    "        model.eval()\n",
    "        sample_inputs, sample_labels = next(iter(val_loader))\n",
    "        sample_inputs = sample_inputs[:8].to(device)\n",
    "        sample_labels = sample_labels[:8].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(sample_inputs)\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "        \n",
    "        # Log sample images with predictions\n",
    "        sample_images = []\n",
    "        for i in range(len(sample_inputs)):\n",
    "            img = sample_inputs[i].cpu().numpy().transpose(1, 2, 0)\n",
    "            img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0,1]\n",
    "            sample_images.append(wandb.Image(\n",
    "                img,\n",
    "                caption=f\"Pred: {class_names[preds[i]]}, True: {class_names[sample_labels[i]]}\"\n",
    "            ))\n",
    "        \n",
    "        metrics[\"samples\"] = sample_images\n",
    "    \n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = f\"vit_model_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_acc,\n",
    "            'train_accuracy': train_acc,\n",
    "            'config': config\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Create and log model artifact\n",
    "        model_artifact = wandb.Artifact(\n",
    "            f\"vit-fer2013-model\",\n",
    "            type=\"model\",\n",
    "            description=f\"Vision Transformer trained on FER2013 - Epoch {epoch+1} - Val Acc: {val_acc:.2f}%\",\n",
    "            metadata={\"val_accuracy\": val_acc, \"epoch\": epoch + 1, **config}\n",
    "        )\n",
    "        model_artifact.add_file(checkpoint_path)\n",
    "        wandb.log_artifact(model_artifact)\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            os.remove(checkpoint_path)\n",
    "            \n",
    "        print(f\"Saved best model at epoch {epoch+1} with val acc: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38513bc",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating on test set...\")\n",
    "# Load the best model from wandb if available, otherwise use local\n",
    "best_model_path = \"best_vit_model.pth\"\n",
    "if not os.path.exists(best_model_path):\n",
    "    # Try to download the best model from wandb\n",
    "    api = wandb.Api()\n",
    "    try:\n",
    "        artifact = api.artifact(f'{wandb.run.entity}/{wandb.run.project}/model-vit-fer2013-model:latest')\n",
    "        artifact_dir = artifact.download()\n",
    "        best_model_path = os.path.join(artifact_dir, os.listdir(artifact_dir)[0])\n",
    "    except:\n",
    "        print(\"Could not load model from wandb, using local model if exists\")\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']} with val acc: {checkpoint['val_accuracy']:.2f}%\")\n",
    "else:\n",
    "    print(\"No saved model found, using current model\")\n",
    "\n",
    "test_loss, test_acc, test_preds, test_labels = validate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification report\n",
    "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "# Log test results to wandb\n",
    "test_metrics = {\n",
    "    \"test/loss\": test_loss,\n",
    "    \"test/accuracy\": test_acc,\n",
    "    \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        preds=test_preds,\n",
    "        y_true=test_labels,\n",
    "        class_names=class_names,\n",
    "        title=\"Test Confusion Matrix\"\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e74715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log classification report\n",
    "report = classification_report(test_labels, test_preds, target_names=class_names, output_dict=True)\n",
    "wandb.log({\"test/classification_report\": wandb.Table(\n",
    "    data=[[k, v[\"precision\"], v[\"recall\"], v[\"f1-score\"], v[\"support\"]] \n",
    "         for k, v in report.items() if k in class_names],\n",
    "    columns=[\"class\", \"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    ")})\n",
    "\n",
    "# Log per-class metrics\n",
    "for cls in class_names:\n",
    "    if cls in report:\n",
    "        test_metrics.update({\n",
    "            f\"test/{cls}/precision\": report[cls][\"precision\"],\n",
    "            f\"test/{cls}/recall\": report[cls][\"recall\"],\n",
    "            f\"test/{cls}/f1\": report[cls][\"f1-score\"]\n",
    "        })\n",
    "\n",
    "wandb.log(test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log final model as artifact\n",
    "final_model_path = \"final_vit_model.pth\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'test_accuracy': test_acc,\n",
    "    'config': config\n",
    "}, final_model_path)\n",
    "\n",
    "final_artifact = wandb.Artifact(\n",
    "    \"vit-fer2013-final-model\",\n",
    "    type=\"model\",\n",
    "    description=\"Final Vision Transformer model trained on FER2013\",\n",
    "    metadata={\"test_accuracy\": test_acc, **config}\n",
    ")\n",
    "final_artifact.add_file(final_model_path)\n",
    "wandb.log_artifact(final_artifact)\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(final_model_path):\n",
    "    os.remove(final_model_path)\n",
    "\n",
    "# Log hyperparameter optimization summary\n",
    "wandb.define_metric(\"val/accuracy\", summary=\"max\")\n",
    "wandb.define_metric(\"val/loss\", summary=\"min\")\n",
    "wandb.define_metric(\"train/accuracy\", summary=\"max\")\n",
    "wandb.define_metric(\"train/loss\", summary=\"min\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the run as completed\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
