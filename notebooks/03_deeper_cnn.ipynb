{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import wandb\n",
    "import time\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57813625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration - Building upon simple CNN lessons\n",
    "CONFIG = {\n",
    "    'model_name': 'deeper_cnn_bn',\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 40,  # More epochs since we expect slower convergence\n",
    "    'image_size': 48,\n",
    "    'num_classes': 7,\n",
    "    'random_seed': 42,\n",
    "    'weight_decay': 1e-4,  # L2 regularization\n",
    "    'dropout_rate': 0.5\n",
    "}\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(CONFIG['random_seed'])\n",
    "np.random.seed(CONFIG['random_seed'])\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"facial-expression-recognition\",\n",
    "    name=f\"{CONFIG['model_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    config=CONFIG,\n",
    "    job_type=\"training\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cebb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reuse dataset class from previous experiment\n",
    "class FERDataset(Dataset):\n",
    "    def __init__(self, dataframe, indices, transform=None):\n",
    "        self.data = dataframe.iloc[indices].reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pixels = self.data.iloc[idx]['pixels']\n",
    "        image = np.array(pixels.split(), dtype=np.uint8).reshape(48, 48)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).unsqueeze(0)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = int(self.data.iloc[idx]['emotion'])\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee2cb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Deeper CNN with Batch Normalization\n",
    "class DeeperCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
    "        super(DeeperCNN, self).__init__()\n",
    "        \n",
    "        # First conv block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Second conv block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Third conv block\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Fourth conv block\n",
    "        self.conv7 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Block 1: 48x48 -> 24x24\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Block 2: 24x24 -> 12x12\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Block 3: 12x12 -> 6x6\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Block 4: 6x6 -> 3x3\n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Global average pooling: 3x3 -> 1x1\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5be33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_indices = np.load('train_indices.npy')\n",
    "val_indices = np.load('val_indices.npy')\n",
    "\n",
    "# Create datasets with same transforms as baseline for fair comparison\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = FERDataset(train_df, train_indices, transform=train_transform)\n",
    "val_dataset = FERDataset(train_df, val_indices, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bfcb2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab1bf4",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40caed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = DeeperCNN(num_classes=CONFIG['num_classes'], dropout_rate=CONFIG['dropout_rate']).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "print(f\"Parameter increase vs Simple CNN: ~{total_params/18000:.1f}x\")  # Approximate simple CNN params\n",
    "\n",
    "# Calculate class weights for balanced loss\n",
    "train_labels = [train_df.iloc[i]['emotion'] for i in train_indices]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = len(train_labels) / (len(class_counts) * class_counts)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed701db5",
   "metadata": {},
   "source": [
    "## Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define loss and optimizer with weight decay\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Log model info\n",
    "wandb.log({\n",
    "    \"model_parameters\": total_params,\n",
    "    \"trainable_parameters\": trainable_params,\n",
    "    \"class_weights\": class_weights.tolist()\n",
    "})\n",
    "\n",
    "# Training function with gradient clipping\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation function (same as before)\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_targets\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "learning_rates = []\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "early_stop_patience = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_targets = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_accuracy': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_acc,\n",
    "        'epoch_time': epoch_time,\n",
    "        'learning_rate': current_lr\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{CONFIG[\"epochs\"]}]')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(f'LR: {current_lr:.6f}, Time: {epoch_time:.2f}s')\n",
    "    print('-' * 60)\n",
    "    \n",
    "    # Save best model and early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'deeper_cnn_best.pth')\n",
    "        print(f'New best validation accuracy: {best_val_acc:.2f}%')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        \n",
    "    # Early stopping\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "actual_epochs = epoch + 1\n",
    "\n",
    "print(f'\\nTraining completed in {total_time:.2f}s ({actual_epochs} epochs)')\n",
    "print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "\n",
    "# Comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Loss curves\n",
    "axes[0,0].plot(train_losses, label='Train Loss', color='blue', alpha=0.7)\n",
    "axes[0,0].plot(val_losses, label='Val Loss', color='red', alpha=0.7)\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].set_title('Training and Validation Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0,1].plot(train_accs, label='Train Acc', color='blue', alpha=0.7)\n",
    "axes[0,1].plot(val_accs, label='Val Acc', color='red', alpha=0.7)\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Accuracy (%)')\n",
    "axes[0,1].set_title('Training and Validation Accuracy')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting indicator\n",
    "acc_gap = np.array(train_accs) - np.array(val_accs)\n",
    "axes[1,0].plot(acc_gap, label='Train - Val Accuracy', color='green', alpha=0.7)\n",
    "axes[1,0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1,0].axhline(y=10, color='red', linestyle='--', alpha=0.5, label='Overfitting Threshold')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Accuracy Gap (%)')\n",
    "axes[1,0].set_title('Overfitting Indicator')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[1,1].plot(learning_rates, label='Learning Rate', color='orange', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Learning Rate')\n",
    "axes[1,1].set_title('Learning Rate Schedule')\n",
    "axes[1,1].set_yscale('log')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('deeper_cnn_training_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Load best model for evaluation\n",
    "model.load_state_dict(torch.load('deeper_cnn_best.pth'))\n",
    "final_val_loss, final_val_acc, final_preds, final_targets = validate_epoch(\n",
    "    model, val_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f'Final validation accuracy: {final_val_acc:.2f}%')\n",
    "\n",
    "# Detailed analysis\n",
    "expression_mapping = {\n",
    "    0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', \n",
    "    4: 'Sad', 5: 'Surprise', 6: 'Neutral'\n",
    "}\n",
    "\n",
    "class_names = [expression_mapping[i] for i in range(7)]\n",
    "class_report = classification_report(final_targets, final_preds, target_names=class_names, output_dict=True)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(final_targets, final_preds, target_names=class_names))\n",
    "\n",
    "# Enhanced confusion matrix\n",
    "cm = confusion_matrix(final_targets, final_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create normalized confusion matrix\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot both absolute and normalized\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
    "ax1.set_title('Confusion Matrix - Absolute Values')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "ax2.set_title('Confusion Matrix - Normalized')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('deeper_cnn_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Per-class performance analysis\n",
    "per_class_acc = []\n",
    "per_class_precision = []\n",
    "per_class_recall = []\n",
    "\n",
    "for i in range(7):\n",
    "    class_acc = class_report[class_names[i]]['precision'] * 100\n",
    "    class_precision = class_report[class_names[i]]['precision'] * 100\n",
    "    class_recall = class_report[class_names[i]]['recall'] * 100\n",
    "    \n",
    "    per_class_acc.append(class_acc)\n",
    "    per_class_precision.append(class_precision)\n",
    "    per_class_recall.append(class_recall)\n",
    "    \n",
    "    print(f'{class_names[i]}: Precision {class_precision:.1f}%, Recall {class_recall:.1f}%')\n",
    "\n",
    "# Performance comparison visualization\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "bars1 = ax.bar(x - width, per_class_precision, width, label='Precision', alpha=0.8)\n",
    "bars2 = ax.bar(x, per_class_recall, width, label='Recall', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, [class_report[name]['f1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
