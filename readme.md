# áƒ¡áƒáƒ®áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ›áƒ”áƒ¢áƒ§áƒ•áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ›áƒáƒªáƒœáƒáƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ¬áƒ•áƒ”áƒ•áƒ

áƒ”áƒ¡ áƒ áƒ”áƒáƒáƒ–áƒ˜áƒ¢áƒáƒ áƒ˜áƒ áƒ›áƒáƒ˜áƒªáƒáƒ•áƒ¡ áƒ©áƒ”áƒ›áƒ¡ áƒ˜áƒ›áƒáƒšáƒ”áƒ›áƒ”áƒœáƒ¢áƒáƒªáƒ˜áƒáƒ¡ Kaggle-áƒ˜áƒ¡ [Challenges in Representation Learning: Facial Expression Recognition Challenge](https://www.kaggle.com/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge) áƒ™áƒáƒœáƒ™áƒ£áƒ áƒ¡áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡. áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜áƒ¡ áƒ›áƒ˜áƒ–áƒáƒœáƒ˜áƒ áƒ¦áƒ áƒ›áƒ áƒ¡áƒ¬áƒáƒ•áƒšáƒ˜áƒ¡ áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ¥áƒ›áƒœáƒ áƒ“áƒ áƒ¨áƒ”áƒ¤áƒáƒ¡áƒ”áƒ‘áƒ, áƒ áƒáƒ›áƒšáƒ”áƒ‘áƒ˜áƒª áƒ™áƒšáƒáƒ¡áƒ˜áƒ¤áƒ˜áƒªáƒ˜áƒ áƒ”áƒ‘áƒ”áƒœ áƒ¡áƒáƒ®áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ›áƒ”áƒ¢áƒ§áƒ•áƒ”áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ¡ áƒ¨áƒ•áƒ˜áƒ“áƒ˜ áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ áƒ”áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒáƒ“.

## ğŸ“‹ áƒ¨áƒ˜áƒœáƒáƒáƒ áƒ¡áƒ˜

* [áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜áƒ¡ áƒ›áƒ˜áƒ›áƒáƒ®áƒ˜áƒšáƒ•áƒ](#-áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜áƒ¡-áƒ›áƒ˜áƒ›áƒáƒ®áƒ˜áƒšáƒ•áƒ)
* [áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒœáƒáƒ™áƒ áƒ”áƒ‘áƒ˜](#-áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ-áƒœáƒáƒ™áƒ áƒ”áƒ‘áƒ˜)
* [áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ áƒ“áƒ áƒ˜áƒœáƒ¡áƒ¢áƒáƒšáƒáƒªáƒ˜áƒ](#-áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ-áƒ“áƒ-áƒ˜áƒœáƒ¡áƒ¢áƒáƒšáƒáƒªáƒ˜áƒ)
* [áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ](#-áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜áƒ¡-áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ)
* [áƒ›áƒ”áƒ—áƒáƒ“áƒáƒšáƒáƒ’áƒ˜áƒ](#-áƒ›áƒ”áƒ—áƒáƒ“áƒáƒšáƒáƒ’áƒ˜áƒ)
* [áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜ áƒ“áƒ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜](#-áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜-áƒ“áƒ-áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜)
* [Weights & Biases áƒ˜áƒœáƒ¢áƒ”áƒ’áƒ áƒáƒªáƒ˜áƒ](#-weights--biases-áƒ˜áƒœáƒ¢áƒ”áƒ’áƒ áƒáƒªáƒ˜áƒ)
* [áƒ›áƒ˜áƒ’áƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜ áƒ“áƒ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜](#-áƒ›áƒ˜áƒ’áƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜-áƒ“áƒ-áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜)
* [áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ](#-áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ)
* [áƒ¨áƒ”áƒ£áƒ”áƒ áƒ—áƒ“áƒ˜ áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ¡](#-áƒ¨áƒ”áƒ£áƒ”áƒ áƒ—áƒ“áƒ˜-áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ¡)
* [áƒšáƒ˜áƒªáƒ”áƒœáƒ–áƒ˜áƒ](#-áƒšáƒ˜áƒªáƒ”áƒœáƒ–áƒ˜áƒ)

## ğŸŒŸ áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜áƒ¡ áƒ›áƒ˜áƒ›áƒáƒ®áƒ˜áƒšáƒ•áƒ

áƒ”áƒ¡ áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜ áƒ˜áƒ™áƒ•áƒšáƒ”áƒ•áƒ¡ áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ áƒœáƒ”áƒ˜áƒ áƒáƒœáƒ£áƒš áƒ¥áƒ¡áƒ”áƒšáƒ˜áƒ¡ áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒáƒ¡ áƒ¡áƒáƒ®áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ›áƒ”áƒ¢áƒ§áƒ•áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ›áƒáƒ¡áƒáƒªáƒœáƒáƒ‘áƒáƒ“, áƒ§áƒ£áƒ áƒáƒ“áƒ¦áƒ”áƒ‘áƒ áƒ’áƒáƒ›áƒáƒ®áƒ•áƒ˜áƒšáƒ”áƒ‘áƒ£áƒšáƒ˜áƒ áƒ°áƒ˜áƒáƒ”áƒ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜áƒ¡áƒ áƒ“áƒ áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ£áƒšáƒ˜ áƒ’áƒáƒ“áƒáƒ¬áƒ§áƒ•áƒ”áƒ¢áƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ•áƒšáƒ”áƒœáƒáƒ–áƒ” áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ›áƒ£áƒ¨áƒáƒáƒ‘áƒáƒ–áƒ”. áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜ áƒáƒ’áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ PyTorch-áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ˜áƒ—, áƒ®áƒáƒšáƒ áƒ§áƒ•áƒ”áƒšáƒ áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒšáƒáƒ’áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ Weights & Biases áƒáƒšáƒáƒ¢áƒ¤áƒáƒ áƒ›áƒáƒ–áƒ” áƒ¡áƒ áƒ£áƒšáƒ˜ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜áƒ¡áƒ áƒ“áƒ áƒ•áƒ˜áƒ–áƒ£áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡.

## ğŸ“Š áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒœáƒáƒ™áƒ áƒ”áƒ‘áƒ˜

áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒœáƒáƒ™áƒ áƒ”áƒ‘áƒ˜ áƒ¨áƒ”áƒ“áƒ’áƒ”áƒ‘áƒ 48x48 áƒáƒ˜áƒ¥áƒ¡áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒ áƒ”áƒ˜áƒ¡áƒ™áƒ”áƒ˜áƒš áƒ¡áƒáƒ®áƒ”áƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ¡áƒáƒ®áƒ£áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡áƒ’áƒáƒœ, áƒ—áƒ˜áƒ—áƒáƒ”áƒ£áƒšáƒ˜ áƒšáƒ”áƒ˜áƒ‘áƒšáƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ áƒ”áƒ áƒ—-áƒ”áƒ áƒ—áƒ˜ áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’áƒ˜ áƒ”áƒ›áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ™áƒáƒ¢áƒ”áƒ’áƒáƒ áƒ˜áƒ˜áƒ—:

* 0: áƒ’áƒáƒ‘áƒ áƒáƒ–áƒ”áƒ‘áƒ (Angry)
* 1: áƒ–áƒ˜áƒ–áƒ¦áƒ˜ (Disgust)
* 2: áƒ¨áƒ˜áƒ¨áƒ˜ (Fear)
* 3: áƒ¡áƒ˜áƒ®áƒáƒ áƒ£áƒšáƒ˜ (Happy)
* 4: áƒ¡áƒ”áƒ•áƒ“áƒ (Sad)
* 5: áƒ’áƒáƒáƒªáƒ”áƒ‘áƒ (Surprise)
* 6: áƒœáƒ”áƒ˜áƒ¢áƒ áƒáƒšáƒ£áƒ áƒ˜ (Neutral)

## ğŸ› ï¸ áƒ“áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ áƒ“áƒ áƒ˜áƒœáƒ¡áƒ¢áƒáƒšáƒáƒªáƒ˜áƒ

1. áƒ“áƒáƒáƒ™áƒšáƒáƒœáƒ”áƒ— áƒ áƒ”áƒáƒáƒ–áƒ˜áƒ¢áƒáƒ áƒ˜áƒ:

   ```bash
   git clone https://github.com/your-username/Facial-Expression-Recognition.git
   cd Facial-Expression-Recognition
   ```

2. áƒ“áƒáƒáƒ§áƒ”áƒœáƒ”áƒ— áƒ¡áƒáƒ­áƒ˜áƒ áƒ áƒáƒáƒ™áƒ”áƒ¢áƒ”áƒ‘áƒ˜:

   ```bash
   pip install -r requirements.txt
   ```

3. áƒ“áƒáƒáƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ”áƒ— Weights & Biases:

   ```bash
   wandb login
   ```

## ğŸ“ áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜áƒ¡ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ

```
Facial-Expression-Recognition/
â”œâ”€â”€ data/                    # áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒœáƒáƒ®áƒ˜ áƒ¡áƒáƒ¥áƒáƒ¦áƒáƒšáƒ“áƒ”
â”œâ”€â”€ models/                  # áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ”áƒ‘áƒ˜
â”‚   â”œâ”€â”€ cnn.py
â”‚   â”œâ”€â”€ resnet.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ notebooks/               # Jupyter áƒœáƒáƒ£áƒ—áƒ‘áƒ£áƒ¥áƒ”áƒ‘áƒ˜ áƒ”áƒ¥áƒ¡áƒáƒšáƒáƒ áƒáƒªáƒ˜áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
â”œâ”€â”€ src/                     # áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ áƒ™áƒáƒ“áƒ˜
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ configs/                 # áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ£áƒ áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¤áƒáƒ˜áƒšáƒ”áƒ‘áƒ˜
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ§  áƒ›áƒ”áƒ—áƒáƒ“áƒáƒšáƒáƒ’áƒ˜áƒ

áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜ áƒ›áƒ˜áƒ°áƒ§áƒ•áƒ”áƒ‘áƒ áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒœáƒ•áƒ˜áƒ—áƒáƒ áƒ”áƒ‘áƒáƒ¡ áƒ”áƒ¢áƒáƒáƒáƒ‘áƒ áƒ˜áƒ•áƒáƒ“:

1. **áƒ¡áƒáƒ¬áƒ§áƒ˜áƒ¡áƒ˜ áƒ›áƒáƒ“áƒ”áƒšáƒ˜**: áƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒ˜ CNN áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ
2. **áƒ›áƒáƒ¬áƒ˜áƒœáƒáƒ•áƒ” áƒ™áƒáƒ›áƒáƒšáƒ”áƒ¥áƒ¡áƒ£áƒ áƒáƒ‘áƒ**: áƒ”áƒ¢áƒáƒáƒáƒ‘áƒ áƒ˜áƒ•áƒáƒ“ áƒ•áƒ–áƒ áƒ“áƒ˜áƒ— áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ˜áƒ¡ áƒ¡áƒ˜áƒ áƒ—áƒ£áƒšáƒ”áƒ¡
3. **áƒ áƒ”áƒ’áƒ£áƒšáƒáƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ**: áƒ¡áƒ®áƒ•áƒáƒ“áƒáƒ¡áƒ®áƒ•áƒ áƒ¢áƒ”áƒ¥áƒœáƒ˜áƒ™áƒ áƒ’áƒáƒ“áƒáƒ›áƒ”áƒ¢áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ—áƒáƒ•áƒ˜áƒ“áƒáƒœ áƒáƒ¡áƒáƒªáƒ˜áƒšáƒ”áƒ‘áƒšáƒáƒ“
4. **áƒ¢áƒ áƒáƒœáƒ¡áƒ¤áƒ”áƒ áƒ£áƒšáƒ˜ áƒ¡áƒ¬áƒáƒ•áƒšáƒ”áƒ‘áƒ**: áƒ¬áƒ˜áƒœáƒáƒ¡áƒ¬áƒáƒ  áƒ’áƒáƒ¬áƒ•áƒ áƒ—áƒœáƒ˜áƒšáƒ˜ áƒ›áƒáƒ“áƒ”áƒšáƒ”áƒ‘áƒ˜áƒ¡ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ
5. **áƒ”áƒœáƒ˜áƒ¡áƒ”áƒ›áƒ‘áƒšáƒ˜**: áƒ›áƒ áƒáƒ•áƒáƒš áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒáƒ”áƒ áƒ—áƒ˜áƒáƒœáƒ”áƒ‘áƒ áƒ£áƒ™áƒ”áƒ—áƒ”áƒ¡áƒ˜ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡

## ğŸ“ˆ áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜ áƒ“áƒ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜

áƒ§áƒ•áƒ”áƒšáƒ áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒšáƒáƒ’áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ Weights & Biases-áƒ¨áƒ˜. áƒšáƒáƒ’áƒ˜áƒ áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ”áƒ‘áƒ˜ áƒ›áƒáƒ˜áƒªáƒáƒ•áƒ¡:

* áƒ¡áƒ¬áƒáƒ•áƒšáƒ”áƒ‘áƒ˜áƒ¡áƒ áƒ“áƒ áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ“áƒáƒœáƒáƒ™áƒáƒ áƒ’áƒ˜ áƒ“áƒ áƒ¡áƒ˜áƒ–áƒ£áƒ¡áƒ¢áƒ”
* áƒ¡áƒáƒ¡áƒ¬áƒáƒ•áƒšáƒ áƒ¡áƒ˜áƒ©áƒ¥áƒáƒ áƒ˜áƒ¡ áƒªáƒ•áƒšáƒáƒ“áƒáƒ‘áƒ
* áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ°áƒ˜áƒáƒ”áƒ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜
* áƒ™áƒáƒœáƒ¤áƒ£áƒ–áƒ˜áƒ˜áƒ¡ áƒ›áƒáƒ¢áƒ áƒ˜áƒªáƒ”áƒ‘áƒ˜
* áƒœáƒ˜áƒ›áƒ£áƒ¨áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ áƒáƒ’áƒœáƒáƒ–áƒ”áƒ‘áƒ˜


# Notebook 01_data_exploration.ipynb
## Data Exploration Results

### Dataset Overview
- **Size**: 28,709 training samples, 7,178 test samples
- **Classes**: 7 facial expressions (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral)
- **Image Format**: 48x48 grayscale images
- **Data Quality**: 0 missing values, 1236 duplicates

### Key Findings
1. **Class Imbalance**: Significant imbalance with ratio 16.55:1 (most:least common)
   - Most common: Happy 
   - Least common: Disgust

2. **Data Characteristics**:
   - Pixel values: 0-255 range
   - Mean pixel value: 131.0 Â± 64.3
   - Consistent image dimensions

3. **Challenges Identified**:
   - Class imbalance will require balanced sampling or weighted loss
   - Low resolution (48x48) limits feature complexity
   - Grayscale only - no color information

### Data Split Strategy
- Training: 22,967 samples (80%)
- Validation: 5,742 samples (20%)  
- Stratified split to maintain class distribution

# Training
## 02_baseline_cnn.ipynb **Baseline CNN**

Simple Convolutional Neural Network for facial expression recognition. Serves as an initial model to establish performance baselines.

### ğŸ”¹ Version 1

#### ğŸ— Architecture

```
Input (1, 48, 48)
â”œâ”€ Conv2d(1, 32, kernel_size=5, padding=2)
â”œâ”€ ReLU()
â”œâ”€ MaxPool2d(kernel_size=2, stride=2)
â”œâ”€ Conv2d(32, 64, kernel_size=5, padding=2)
â”œâ”€ ReLU()
â”œâ”€ MaxPool2d(kernel_size=2, stride=2)
â”œâ”€ Flatten()
â”œâ”€ Dropout(0.3)
â”œâ”€ Linear(64 * 12 * 12, 128)
â”œâ”€ ReLU()
â”œâ”€ Dropout(0.3)
â””â”€ Linear(128, 7)
```

#### âš™ï¸ Training Configuration
- **Optimizer**: Adam (lr=0.001)
- **Loss**: Cross-entropy
- **Batch Size**: 64
- **Epochs**: 20 (early stopping)
- **Regularization**:
  - Dropout (0.3)

#### ğŸ“Š Results
- Training accuracy: ~85%
- Validation accuracy: ~60%
- **Issue**: Overfitting observed after 20 epochs


[Simple_cnn_v1](https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition/runs/70toflci?nw=nwuserellekvirikashvili)

---



### ğŸ”¹ Version 2 (Improved)

#### ğŸ›  Architecture Improvements
- **Batch Normalization** after each conv layer
- **Spatial Dropout (0.2)** in conv layers
- **Higher Dropout (0.6)** in FC layers
- **Reduced FC layers** (512 â†’ 256 â†’ 128)
- **Adaptive Pooling** for better input size handling

#### âš™ï¸ Training Configuration
- **Learning Rate**: 0.0005 (reduced from 0.001)
- **Weight Decay**: 1e-4 (L2 regularization)
- **Early Stopping** with patience=5
- **Learning Rate Scheduling**: Reduce on plateau
- **Batch Size**: 64 (unchanged)

#### ğŸ“Š Expected Improvements
- Better generalization
- Reduced overfitting
- More stable training

## ğŸ“ `03_deeper_cnn.ipynb`

### ğŸ§  Deeper CNN with Batch Normalization

---

## ğŸ”¹ Version 1 (Deep\_CNN\_V1)

### ğŸ— Architecture

* 7-áƒ¨áƒ áƒ˜áƒáƒœáƒ˜ Convolutional áƒœáƒ”áƒ áƒ•áƒ£áƒšáƒ˜ áƒ¥áƒ¡áƒ”áƒšáƒ˜ (CNN)
* **4 Convolutional áƒ‘áƒšáƒáƒ™áƒ˜**, áƒ§áƒáƒ•áƒ”áƒšáƒ˜ áƒ“áƒáƒ¡áƒ áƒ£áƒšáƒ”áƒ‘áƒ£áƒšáƒ˜áƒ MaxPooling-áƒ˜áƒ—
* áƒ§áƒáƒ•áƒ”áƒšáƒ˜ Conv-áƒ¨áƒ áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’ áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜áƒ **Batch Normalization**
* **Global Average Pooling** áƒ¡áƒ áƒ£áƒšáƒ“áƒ”áƒ‘áƒ FC áƒ¤áƒ”áƒœáƒ”áƒ‘áƒáƒ›áƒ“áƒ”
* Dropout (0.5) áƒ áƒ”áƒ’áƒ£áƒšáƒáƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
* **Batch Normalization** áƒ’áƒáƒ›áƒáƒ˜áƒ§áƒ”áƒœáƒ”áƒ‘áƒ áƒ áƒáƒ’áƒáƒ áƒª Convolutional, áƒ˜áƒ¡áƒ” Fully Connected áƒ¤áƒ”áƒœáƒ”áƒ‘áƒ¨áƒ˜

### âš™ï¸ áƒ°áƒ˜áƒáƒ”áƒ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜

* Filters: `32 â†’ 64 â†’ 128 â†’ 256`
* Optimizer: **Adam**, learning rate = `0.001`
* L2 weight decay: `1e-4`
* Epochs: `40` (Early stopping áƒ¨áƒ”áƒ¡áƒáƒ«áƒšáƒ”áƒ‘áƒ”áƒšáƒ˜)
* Dropout rate: `0.5`
* Tracking: Weights & Biases áƒ˜áƒœáƒ¢áƒ”áƒ’áƒ áƒáƒªáƒ˜áƒ (`wandb`)

### ğŸ“‰ Performance

* **Train Loss**: `0.1689`, **Train Accuracy**: `93.93%`
* **Val Loss**: `2.3061`, **Val Accuracy**: `57.72%`
* **Early Stopping**: áƒ’áƒáƒœáƒ®áƒáƒ áƒªáƒ˜áƒ”áƒšáƒ“áƒ **24-áƒ” áƒ”áƒáƒáƒ¥áƒáƒ–áƒ”**
* **Observation**: áƒ›áƒáƒ“áƒ”áƒšáƒ›áƒ áƒ’áƒáƒ“áƒáƒ­áƒáƒ áƒ‘áƒ”áƒ‘áƒ£áƒšáƒáƒ“ áƒ›áƒáƒ”áƒ áƒ’áƒ áƒ¢áƒ áƒ”áƒœáƒ˜áƒœáƒ’ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ¡ â€” **overfitting**

ğŸ“Š [Deep\_cnn\_v1 Run on W\&B](https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition/runs/f1pw8dnp?nw=nwuserellekvirikashvili)

---

## ğŸ”¹ Version 2 (Deep\_CNN\_V2)

### âš™ï¸ Key Changes from V1

* **Epochs áƒ¨áƒ”áƒ›áƒªáƒ˜áƒ áƒ“áƒ**: `40 â†’ 30`
* **Dropout áƒ’áƒáƒ˜áƒ–áƒáƒ áƒ“áƒ**: `0.5 â†’ 0.7`
* **Spatial Dropout áƒ“áƒáƒ›áƒáƒ¢áƒ”áƒ‘áƒ£áƒšáƒ˜áƒ**:
  `self.dropout1 = nn.Dropout2d(0.1)` â€” áƒáƒ“áƒ áƒ”áƒ£áƒš áƒšáƒ”áƒ˜áƒ”áƒ áƒ”áƒ‘áƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡, feature-level áƒ áƒ”áƒ’áƒ£áƒšáƒáƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡áƒ—áƒ•áƒ˜áƒ¡
* **Channel áƒ™áƒáƒ›áƒáƒ áƒ”áƒ¡áƒ˜áƒ”áƒ‘áƒ˜ áƒ‘áƒáƒšáƒ Conv áƒ¤áƒ”áƒœáƒ”áƒ‘áƒ¨áƒ˜**: `256 â†’ 192`
* **FC áƒ¤áƒ”áƒœáƒ”áƒ‘áƒ˜ áƒ’áƒáƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒ“áƒ**: `512 â†’ 256`
* **Early Stopping áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜ áƒ’áƒáƒ›áƒ™áƒáƒªáƒ áƒ“áƒ**:

  ```python
  'early_stop_patience': 7,  # More aggressive early stopping
  'lr_patience': 3,          # Reduce LR sooner
  ```

### ğŸ¯ Goal

* **Overfitting-áƒ˜áƒ¡ áƒ¨áƒ”áƒ›áƒªáƒ˜áƒ áƒ”áƒ‘áƒ**
* **áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ’áƒ”áƒœáƒ”áƒ áƒáƒšáƒ˜áƒ–áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ’áƒáƒ£áƒ›áƒ¯áƒáƒ‘áƒ”áƒ¡áƒ”áƒ‘áƒ**
* **áƒ›áƒ”áƒ¢áƒáƒ“ áƒ¡áƒ¬áƒ áƒáƒ¤áƒ˜ áƒ“áƒ áƒáƒ’áƒ áƒ”áƒ¡áƒ˜áƒ£áƒšáƒ˜ áƒáƒ“áƒáƒáƒ¢áƒáƒªáƒ˜áƒ validation performance-áƒ–áƒ”**

### â³ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜áƒ¡ áƒ›áƒáƒšáƒáƒ“áƒ˜áƒœáƒ˜

* áƒ£áƒ™áƒ”áƒ—áƒ”áƒ¡áƒ˜ generalization-validation áƒ‘áƒáƒšáƒáƒœáƒ¡áƒ˜
* áƒœáƒáƒ™áƒšáƒ”áƒ‘áƒ˜ variance epochs-áƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡
* áƒœáƒáƒ™áƒšáƒ”áƒ‘áƒ˜ training-validation gap


#### ğŸ“Š Results

* **Train Loss**: `0.1689`, **Train Accuracy**: `93.93%`
* **Val Loss**: `2.3061`, **Val Accuracy**: `57.72%`
* **Early Stopping**: áƒ’áƒáƒœáƒ®áƒáƒ áƒªáƒ˜áƒ”áƒšáƒ“áƒ **24-áƒ” áƒ”áƒáƒáƒ¥áƒáƒ–áƒ”**


[Deeper_cnn_v2](https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/facial-expression-recognition/runs/ql1dpugq?nw=nwuserellekvirikashvili)


### V3

#### Basic Flow for Augmented Training:
`Tensor (from dataset) â†’ ToPILImage() â†’ PIL Augmentations â†’ ToTensor() â†’ Tensor Augmentations â†’ Normalize`


#### Features:

Â±15Â° random rotation
50% horizontal flip probability
Random translation (Â±10% of image size)
Random scaling (90%-110%)
Random shear transformation
Brightness/contrast jitter
Random erasing (10% probability)
---

## âœ… áƒ“áƒáƒ¡áƒ™áƒ•áƒœáƒ

> áƒáƒ˜áƒ áƒ•áƒ”áƒšáƒ˜ áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ áƒ¡áƒáƒ™áƒ›áƒáƒ áƒ˜áƒ¡áƒáƒ“ áƒ«áƒšáƒ˜áƒ”áƒ áƒ˜ áƒáƒ¦áƒ›áƒáƒ©áƒœáƒ“áƒ training áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ–áƒ”, áƒ›áƒáƒ’áƒ áƒáƒ› áƒ•áƒ”áƒ  áƒ’áƒáƒáƒ áƒ—áƒ•áƒ áƒ—áƒáƒ•áƒ˜ validation-áƒ–áƒ” â€” áƒ¡áƒáƒ­áƒ˜áƒ áƒ áƒ’áƒáƒ®áƒ“áƒ áƒáƒ’áƒ áƒ”áƒ¡áƒ˜áƒ£áƒšáƒ˜ áƒ áƒ”áƒ’áƒ£áƒšáƒáƒ áƒ˜áƒ–áƒáƒªáƒ˜áƒ áƒ“áƒ áƒáƒ“áƒ áƒ”áƒ£áƒšáƒ˜ learning rate decay.
> áƒ›áƒ”áƒáƒ áƒ” áƒ•áƒ”áƒ áƒ¡áƒ˜áƒ áƒ›áƒ˜áƒ“áƒ˜áƒ¡ **leaner architecture + smarter regularization** áƒ¡áƒ¢áƒ áƒáƒ¢áƒ”áƒ’áƒ˜áƒ˜áƒ—, áƒ áƒáƒ—áƒ áƒ“áƒáƒ˜áƒ‘áƒáƒšáƒáƒœáƒ¡áƒáƒ¡ áƒ¡áƒ˜áƒ¡áƒ¬áƒ áƒáƒ¤áƒ”, áƒ¡áƒ˜áƒ–áƒ£áƒ¡áƒ¢áƒ” áƒ“áƒ áƒ¡áƒ¢áƒáƒ‘áƒ˜áƒšáƒ£áƒ áƒáƒ‘áƒ.


## 04_attention_cnn.ipynb

### ğŸ§  Attention-based CNN Architecture

#### ğŸ” Overview
A Convolutional Neural Network enhanced with Convolutional Block Attention Module (CBAM) that learns to focus on the most discriminative facial regions for expression recognition.

#### ğŸ— Core Architecture

```
Input (1, 48, 48)
â”œâ”€ Conv2d(1, 32) â†’ BatchNorm â†’ ReLU
â”œâ”€ Conv2d(32, 32) â†’ BatchNorm â†’ ReLU â†’ MaxPool2d(2)
â””â”€ CBAM(32)  # First attention block

â”œâ”€ Conv2d(32, 64) â†’ BatchNorm â†’ ReLU
â”œâ”€ Conv2d(64, 64) â†’ BatchNorm â†’ ReLU â†’ MaxPool2d(2)
â””â”€ CBAM(64)  # Second attention block

â”œâ”€ Conv2d(64, 128) â†’ BatchNorm â†’ ReLU
â”œâ”€ Conv2d(128, 128) â†’ BatchNorm â†’ ReLU â†’ MaxPool2d(2)
â””â”€ CBAM(128)  # Third attention block

â”œâ”€ AdaptiveAvgPool2d(1)
â”œâ”€ Flatten
â”œâ”€ Linear(128, 256) â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5)
â””â”€ Linear(256, 7)  # 7 emotion classes
```

#### ğŸ¯ Attention Mechanism (CBAM)

**Convolutional Block Attention Module** combines:

1. **Channel Attention**
   - Captures 'what' to focus on in the feature maps
   - Uses both average and max pooling paths
   - Learns channel-wise feature importance

2. **Spatial Attention**
   - Determines 'where' to focus in the spatial dimensions
   - Applies 1x1 convolutions to create spatial attention maps
   - Highlights important facial regions for expression recognition

#### âš™ï¸ Training Configuration
- **Optimizer**: AdamW with weight decay (1e-4)
- **Learning Rate**: 0.001 with ReduceLROnPlateau scheduling
- **Regularization**:
  - Dropout (0.5) in fully connected layers
  - L2 weight decay
  - Data augmentation (random horizontal flip, rotation)
- **Batch Size**: 64
- **Epochs**: 50 with early stopping

#### ğŸ“Š Performance Features
- **Visual Attention Maps**: Visualize which facial regions the model focuses on
- **Class Activation Mapping**: Understand model decisions
- **W&B Integration**: Track experiments and compare runs
- **Confusion Matrix**: Detailed performance analysis

#### ğŸš€ Key Benefits
1. **Improved Accuracy**: Focuses on relevant facial features
2. **Better Generalization**: Attention acts as a form of regularization
3. **Interpretability**: Visual explanations of model decisions
4. **Efficiency**: Lightweight attention modules with minimal computational overhead

#### ğŸ›  Implementation Details
- Uses PyTorch for model implementation
- Integrates with Weights & Biases for experiment tracking
- Includes comprehensive data augmentation
- Implements learning rate scheduling and early stopping

#### ğŸ“ˆ Expected Performance
- Training Accuracy: ~85-90%
- Validation Accuracy: ~60-65%
- Focuses on eyes, mouth, and eyebrow regions for expression recognition

## ğŸ” Weights & Biases áƒ˜áƒœáƒ¢áƒ”áƒ’áƒ áƒáƒªáƒ˜áƒ

áƒ§áƒ•áƒ”áƒšáƒ áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ˜ áƒšáƒáƒ’áƒ˜áƒ áƒ“áƒ”áƒ‘áƒ Weights & Biases-áƒ¨áƒ˜ áƒ¨áƒ”áƒ›áƒ“áƒ”áƒ’áƒ˜ áƒ¡áƒ¢áƒ áƒ£áƒ¥áƒ¢áƒ£áƒ áƒ˜áƒ—:

* áƒáƒ áƒáƒ”áƒ¥áƒ¢áƒ˜: `facial-expression-recognition`
* áƒ¢áƒ”áƒ’áƒ”áƒ‘áƒ˜: `[model_type, dataset_version, experiment_type]`
* áƒ™áƒáƒœáƒ¤áƒ˜áƒ’áƒ˜: áƒ°áƒ˜áƒáƒ”áƒ áƒáƒáƒ áƒáƒ›áƒ”áƒ¢áƒ áƒ”áƒ‘áƒ˜ áƒ“áƒ áƒáƒ áƒ¥áƒ˜áƒ¢áƒ”áƒ¥áƒ¢áƒ£áƒ áƒ
* áƒ›áƒ”áƒ¢áƒ áƒ˜áƒ™áƒ”áƒ‘áƒ˜: áƒ¡áƒ¬áƒáƒ•áƒšáƒ”áƒ‘áƒ˜áƒ¡/áƒ•áƒáƒšáƒ˜áƒ“áƒáƒªáƒ˜áƒ˜áƒ¡ áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ”áƒ‘áƒ˜
* áƒáƒ áƒ¢áƒ˜áƒ¤áƒáƒ¥áƒ¢áƒ”áƒ‘áƒ˜: áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¨áƒ”áƒœáƒáƒ®áƒ£áƒšáƒ˜ áƒ¬áƒáƒœáƒ”áƒ‘áƒ˜

## ğŸ“ áƒ›áƒ˜áƒ’áƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜ áƒ“áƒ áƒáƒœáƒáƒšáƒ˜áƒ–áƒ˜

áƒ”áƒ¥áƒ¡áƒáƒ”áƒ áƒ˜áƒ›áƒ”áƒœáƒ¢áƒ”áƒ‘áƒ˜áƒ¡ áƒ«áƒ˜áƒ áƒ˜áƒ—áƒáƒ“áƒ˜ áƒ›áƒ˜áƒ’áƒœáƒ”áƒ‘áƒ”áƒ‘áƒ˜:

1. **áƒ’áƒáƒ“áƒáƒ›áƒ”áƒ¢áƒ”áƒ‘áƒ£áƒšáƒ˜ áƒ›áƒáƒ áƒ’áƒ”áƒ‘áƒ**: áƒ›áƒáƒ’áƒ•áƒáƒ áƒ“áƒ áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ—áƒ áƒáƒ£áƒ’áƒ›áƒ”áƒœáƒ¢áƒáƒªáƒ˜áƒ˜áƒ—áƒ áƒ“áƒ dropout-áƒ˜áƒ—
2. **áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ˜áƒ¡ áƒ“áƒ˜áƒ¡áƒ‘áƒáƒšáƒáƒœáƒ¡áƒ˜**: áƒ’áƒáƒ›áƒáƒ§áƒ”áƒœáƒ”áƒ‘áƒ£áƒšáƒ˜áƒ áƒ¬áƒáƒœáƒ˜áƒáƒœáƒ˜ áƒ“áƒáƒœáƒáƒ™áƒáƒ áƒ’áƒ˜áƒ¡ áƒ¤áƒ£áƒœáƒ¥áƒªáƒ˜áƒ”áƒ‘áƒ˜
3. **áƒ¡áƒáƒ¡áƒ¬áƒáƒ•áƒšáƒ áƒ¡áƒ˜áƒ©áƒ¥áƒáƒ áƒ˜áƒ¡ áƒªáƒ•áƒšáƒ**: áƒ“áƒ˜áƒ“áƒ˜ áƒ’áƒáƒ•áƒšáƒ”áƒœáƒ áƒáƒ¥áƒ•áƒ¡ áƒ™áƒáƒœáƒ•áƒ”áƒ áƒ’áƒ”áƒœáƒªáƒ˜áƒáƒ–áƒ”
4. **áƒ›áƒáƒ“áƒ”áƒšáƒ˜áƒ¡ áƒ¡áƒ˜áƒ¦áƒ áƒ›áƒ”**: áƒ‘áƒáƒšáƒáƒœáƒ¡áƒ˜ áƒ¡áƒ˜áƒ áƒ—áƒ£áƒšáƒ”áƒ¡áƒ áƒ“áƒ áƒ”áƒ¤áƒ”áƒ¥áƒ¢áƒ£áƒ áƒáƒ‘áƒáƒ¡ áƒ¨áƒáƒ áƒ˜áƒ¡


